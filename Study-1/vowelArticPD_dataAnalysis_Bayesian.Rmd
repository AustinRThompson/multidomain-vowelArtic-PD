---
title: "Vowel Artic in PD: Bayesian Analysis"
output: html_notebook
---

# Packages
```{r}
library(tidyverse)
library(ggrepel)
library(extraDistr) # install.packages("extraDistr")
library(HDInterval) # install.packages("HDInterval")
library(tidybayes) # install.packages("tidybayes")
library(bayesplot) # install.packages("bayesplot")
library(modelr)
library(broom.mixed) # install.packages("broom.mixed")
library(brms) # install.packages("brms")
theme_set(theme_minimal())
```

# Background
	
This study is a follow-up analysis of our previous work examining the predictive value of acoustic and kinematic measures in predicting intelligibility and articulatory precision in speakers with and without Parkinson's disease (PD; https://osf.io/hfuq5/). 

Our previous findings indicated that among several measures, vowel space area (VSA), and kinematic distance were the best predictors of both intelligibility and articulatory precision. However, this relationship was examined using pooled data, without considering differences in speaking group (control vs. PD) or condition (conversational vs. more clear). Therefore, it remains unknown how speakers with and without dysarthria due to PD differ in terms of intelligibility, articulatory precision, VSA, and kinematic distance. Additionally, it is unclear how these groups respond to prompts to speak more clearly. Thus, the following research questions were posed:

1. How do speakers with and without PD of varying dysarthria severity levels differ in terms of intelligibility, articulatory precision, vowel space area (VSA), and kinematic distance? (Group Effects)
2. When prompted to speak more clearly, how do speakers with and without PD of varying dysarthria severities modify their articulation (VSA and kinematic distance), and does this cue result in perceptual gains in intelligibility and articulatory precision? (Group Ã— Condition Interaction Effects)


# Data Analysis
Before we can build our models, we'll need some information about the speakers.
```{r}
speakerList <-
  rio::import(file = "../Data/PreppedData/Speaker List_Clean.csv") %>%
  dplyr::select(!`Folder 2`:`Sentence Repetition`) %>%
  
  # Making sure dxTime is numeric
  dplyr::mutate(dxTime = as.numeric(dxTime)) %>%
  
  # Refactoring Sex
  dplyr::mutate(Sex = factor(
    Sex,
    levels = c("M",
               "F"),
    labels = c("Male Speakers",
               "Female Speakers")
  )) %>%
  
  # Removing the MoCA Scores - Not all speakers had them
  dplyr::select(!MoCA)
```

## Intelligibility
Intelligibility was collected through ratings obtained through a listener survey. They would listen to a speaker, and then rate how intelligible the speaker was using a horizontally oriented visual analog scale. The left end of the scale was labeled "Cannot understand anything" and corresponded to a value of 0. The right end of the scale was labeled "Understood everything" and corresponded to a value of 100.

While any value between 0 and 100 could be selected, listeners tended to rate on the extreme ends, resulting in a beta distribution. Therefore, for this model, we used a beta family for our model. But first, we rescaled the intelligibility measure to fall between 0 and 1, excluding the endpoint values of 0 and 1.
### Loading the data
```{r}
perceptualMeasures <- rio::import(file = "../Data/PreppedData/ListenerData/ListenerRatings_allRatings.csv") %>%
  dplyr::select(!V1) %>%
  dplyr::filter(ratingType == "Int") %>% # We only want the intelligibility ratings for this model
  dplyr::filter(condition != "lessClear") %>% # We won't use the less clear condition in this study
  
  # Here we rename some variables and clean up the factors
  dplyr::rename(Int = Rating) %>%
  dplyr::mutate(Sex = factor(Sex, levels = c("M", "F")),
                condition = factor(condition, levels = c("conv", "moreClear"))) %>%
  
  # Now we merge the intelligibility ratings with the speakerList, which has information about severity level.
  base::merge(., speakerList %>%
                dplyr::select(StudyID, Severity)) %>%
  dplyr::mutate(
    Severity = factor(
      Severity,
      levels = c("HC", "Mild", "Moderate", "Severe", "Profound"),
      labels = c("Control", "Mild", "Moderate", "Severe", "Profound")
    )
  )

# Lets visualize the outcome measure, intelligibility
hist(perceptualMeasures$Int)

# Here, we rescale the measure to fit a beta distribtion
epsilon <- 1e-5
modelData <- perceptualMeasures %>%
  dplyr::select(StudyID,
                Group,
                Sex,
                Age,
                Severity,
                condition,
                Sent,
                rep,
                ListenerID,
                Int) %>%
  dplyr::mutate(
    Int = Int / 100,
    # the following makes sure that 0 and 1 are not included in the beta distribution
    Int = Int * ((nrow(.) - 1) + .5) / nrow(.),
    Int = Int * (1 - 2 * epsilon) + epsilon
  )

performance::check_distribution(modelData$Int)

# Taking out the trash
rm(perceptualMeasures, epsilon)
```

### Priors
First, we need to figure out the model parameters
```{r}

brms_Int_modelFormula <-
  Int ~ Severity * condition +
  (1 | StudyID / Sent / rep) + # Each Speaker (StudyID) read three sentences (Sent), five times each (rep)
  (1 | ListenerID)

brms::get_prior(
  brms_Int_modelFormula, 
  data = modelData, 
  family = Beta)
```
Now we can specify weakly informative priors.
```{r}
prior_int <- c(
  prior(normal(0, 10), class = Intercept), # start with larger value - 10
  prior(normal(0, 10), class = b),
  prior(cauchy(0, 10), class = sd), # paper on variance priors: https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full - check 95% interval for cauchy - very large upper bound 10
  prior(gamma(1, 0.5), class = phi) # Phi = 1, mu = .5
)
```

### Building the model
```{r}
condition_int <- brms::brm(
  formula = brms_Int_modelFormula,
  data = modelData,
  prior = prior_int,
  family = Beta,
  chains = 4,
  cores = 4,
  iter = 4000,
  warmup = 1000,
  control = list(adapt_delta = 0.95),
  file = "Models/brms_Int.rds",
  file_refit = "on_change"
)
```

### Model Summary
```{r}
summary(condition_int)
```
### Model Diagnostics
```{r}
base::plot(condition_int)
pp_check(condition_int, ndraws = 1000)

summary(condition_int)$fixed
plot(conditional_effects(condition_int), ask = FALSE)
```

```{r}
posterior <- as.matrix(condition_int)
fixed_effects <- posterior %>%
  as.data.frame() %>%
  dplyr::select(starts_with("b_")) %>%
  colnames()

mcmc_areas(posterior,
           pars = fixed_effects,
           # arbitrary threshold for shading probability mass
           prob = 0.95) 

```

```{r}
bayestestR::p_direction(condition_int)
```
### Visualizations
```{r}
epsilon <- 1e-5
post_Int_data <- modelData %>%
  data_grid(Severity, condition) %>%
  epred_draws(object = condition_int,
              ndraws = 4000,
              re_formula = NA) %>%
  # Reverse the transformation applied before running the model.
  dplyr::mutate(
    Int = (.epred - epsilon) / (1 - 2 * epsilon),
    # Step 1 & 2: Reverse the offset and scaling
    Int = Int * nrow(modelData) / ((nrow(modelData) - 1) + .5)
  )

post_Int_plot <- post_Int_data %>%
  ggplot() +
  aes(
    y = Int,
    x = Severity,
    group = condition,
    #alpha = condition,
    fill = condition
  ) +
  
  # Density + CrIs
  stat_halfeye(
    alpha = .8
  ) +

  # Rename axes
  labs(
    y = "Intelligibility",
    x = "Group/Severity"
  )
post_Int_plot

int_agg <- modelData %>%
  # Reverse the transformation applied before running the model.
  dplyr::mutate(
    Int = (Int - epsilon) / (1 - 2 * epsilon),
    # Step 1 & 2: Reverse the offset and scaling
    Int = Int * nrow(modelData) / ((nrow(modelData) - 1) + .5)
  ) %>%
  group_by(StudyID, Severity, condition) %>%
  dplyr::summarise(.epred = mean(Int))

final_Int_plot <- post_Int_plot +
  geom_point(
    data = int_agg,
    aes(
      y = .epred,
      x = Severity,
      group = condition,
      fill = condition
    ),
    pch = 21,
    alpha = .5,
    position = position_nudge(x = -.1)
  ) +
  coord_cartesian(ylim = c(0, 1))  +
  labs(fill = "Condition") +
  scale_fill_manual(values = )
final_Int_plot

ggsave(
  plot = final_Int_plot,
  filename = "Plots/brms_Int.png",
  height = 3,
  width = 5,
  units = "in",
  scale = 1,
  bg = "white"
)
```

## Articulatory Precision
Articulatory precision was collected in the same way as intelligibility ratings. They were collected through ratings obtained through a listener survey. They would listen to a speaker, and then rate how precise the speaker was using a horizontally oriented visual analog scale. The left end of the scale was labeled "imprecise or mumbled" and corresponded to a value of 0. The right end of the scale was labeled "precise or clear" and corresponded to a value of 100.

While any value between 0 and 100 could be selected, listeners tended to rate on the extreme ends, resulting in a beta distribution. Therefore, for this model, we used a beta family for our model. But first, we rescaled the precision ratings to fall between 0 and 1, excluding the endpoint values of 0 and 1.

### Loading the data
```{r}
perceptualMeasures <- rio::import(file = "../Data/PreppedData/ListenerData/ListenerRatings_allRatings.csv") %>%
  dplyr::select(!V1) %>%
  dplyr::filter(ratingType == "AP") %>% # We only want the articulatory precision ratings for this model
  dplyr::filter(condition != "lessClear") %>% # We won't use the less clear condition in this study
  
  # Here we rename some variables and clean up the factors
  dplyr::rename(AP = Rating) %>%
  dplyr::mutate(Sex = factor(Sex, levels = c("M", "F")),
                condition = factor(condition, levels = c("conv", "moreClear"))) %>%
  
  # Now we merge the AP ratings with the speakerList, which has information about severity level.
  base::merge(., speakerList %>%
                dplyr::select(StudyID, Severity)) %>%
  dplyr::mutate(
    Severity = factor(
      Severity,
      levels = c("HC", "Mild", "Moderate", "Severe", "Profound"),
      labels = c("Control", "Mild", "Moderate", "Severe", "Profound")
    )
  )


# Lets visualize the outcome measure, intelligibility
hist(perceptualMeasures$AP)

# Here, we rescale the measure to fit a beta distribtion
epsilon <- 1e-5
modelData <- perceptualMeasures %>%
  dplyr::select(StudyID,
                Group,
                Sex,
                Age,
                Severity,
                condition,
                Sent,
                rep,
                ListenerID,
                AP) %>%
  dplyr::mutate(
    AP = AP / 100,
    AP = AP * ((nrow(.) - 1) + .5) / nrow(.),
    AP = AP * (1 - 2 * epsilon) + epsilon
  ) # this makes sure that 0 and 1 are not included in the beta distribution

performance::check_distribution(modelData$AP)

# Taking out the trash
rm(perceptualMeasures, epsilon)
```

### Priors
First, we need to figure out the model parameters
```{r}
brms_AP_modelFormula <- 
  AP ~ Severity*condition + 
    (1 | StudyID/Sent/rep) + # Each Speaker (StudyID) read three sentences (Sent), five times each (rep)
    (1 | ListenerID)

brms::get_prior(
  brms_AP_modelFormula,
  data = modelData,
  family = Beta
)
```

Now we can specify weakly informative priors.
```{r}
prior_AP <- c(
  prior(normal(0, 10), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(cauchy(0, 10), class = sd),
  prior(gamma(1, 0.5), class = phi) # Phi = 1, mu = .5
)
```

### Building the model
```{r}
condition_AP <- brms::brm(
  formula = brms_AP_modelFormula,
  data = modelData,
  prior = prior_AP,
  family = Beta,
  chains = 4,
  cores = 4,
  iter = 4000,
  warmup = 1000,
  control = list(adapt_delta = 0.95),
  file = "Models/brms_AP.rds",
  file_refit = "on_change"
)
```

### Model Summary
```{r}
summary(condition_AP)
```
### Model Diagnostics
```{r}
plot(condition_AP, variables = "^b_")

pp_check(condition_AP, ndraws = 1000)

summary(condition_AP)$fixed
plot(conditional_effects(condition_AP), ask = FALSE)
```

```{r}
posterior <- as.matrix(condition_AP)
fixed_effects <- posterior %>%
  as.data.frame() %>%
  dplyr::select(starts_with("b_")) %>%
  colnames()

mcmc_areas(posterior,
           pars = fixed_effects,
           # arbitrary threshold for shading probability mass
           prob = 0.95) 
```

```{r}
bayestestR::p_direction(condition_AP)
```
### Visualizations
```{r}
epsilon <- 1e-5
post_AP_data <- modelData %>%
  data_grid(Severity, condition) %>%
  epred_draws(object = condition_AP,
              ndraws = 4000,
              re_formula = NA) %>%
  # Reverse the transformation applied before running the model.
  dplyr::mutate(
    AP = (.epred - epsilon) / (1 - 2 * epsilon),
    # Step 1 & 2: Reverse the offset and scaling
    AP = AP * nrow(modelData) / ((nrow(modelData) - 1) + .5)
  )

post_AP_plot <- post_AP_data %>%
  ggplot() +
  aes(
    y = AP,
    x = Severity,
    group = condition,
    fill = condition
  ) +
  
  # Density + CrIs
  stat_halfeye(alpha = .5) +

  # Rename axes
  labs(
    y = "Articulatory Precision",
    x = "Group/Severity"
  )
post_AP_plot

AP_agg <- modelData %>%
  # Reverse the transformation applied before running the model.
  dplyr::mutate(
    AP = (AP - epsilon) / (1 - 2 * epsilon),
    # Step 1 & 2: Reverse the offset and scaling
    AP = AP * nrow(modelData) / ((nrow(modelData) - 1) + .5)
  ) %>%
  group_by(StudyID, Severity, condition) %>%
  dplyr::summarise(AP = mean(AP))

final_AP_plot <- post_AP_plot +
  geom_point(
    data = AP_agg,
    aes(
      y = AP,
    x = Severity,
    group = condition,
    fill = condition),
    pch = 21, alpha = .5,
    position = position_nudge(x = -.1)
  ) +
  coord_cartesian(ylim = c(0,1))  +
  labs(fill = "Condition")
final_AP_plot

ggsave(
  plot = final_AP_plot,
  filename = "Plots/brms_AP.png",
  height = 3,
  width = 5,
  units = "in",
  scale = 1,
  bg = "white"
)
```

## aVSA
Acoustic Vowel Space Area (aVSA) is an acoustic measure of articulatory working space. There are known sex differences when aVSA is measured in Hz. Therefore, we will measure it in Bark to try to reduce the sex effects.

The bark transformed aVSA measure ranges from .8 - 18 Bark. So it is a measure bound by 0, with no negative values. For this reason, we will use a lognormal family.
### Loading the data
```{r}
vsaMeasures <- rio::import(file = "../Data/PreppedData/CollatedData/TargetMeasures_vsaMeasures.csv") %>%
  dplyr::filter(condition != "lessClear") %>% # We won't use the less clear condition in this study
  
  # Here we rename some variables and clean up the factors
  dplyr::mutate(Sex = factor(Sex, levels = c("M", "F")),
                condition = factor(condition, levels = c("conv", "moreClear"))) %>%
  
  # Now we merge the data with the speakerList, which has information about severity level.
  base::merge(., speakerList %>%
                dplyr::select(StudyID, Severity)) %>%
  dplyr::mutate(
    Severity = factor(
      Severity,
      levels = c("HC", "Mild", "Moderate", "Severe", "Profound"),
      labels = c("Control", "Mild", "Moderate", "Severe", "Profound")
    )
  )

# Lets visualize the outcome measure, aVSA_bark
hist(vsaMeasures$aVSA_bark)

modelData <- vsaMeasures
performance::check_distribution(modelData$aVSA_bark)

# Taking out the trash
rm(vsaMeasures)
```

### Priors
First, we need to figure out the model parameters
```{r}
brms_aVSA_modelFormula <- 
  aVSA_bark ~ 
  Severity * condition +
  (1 | StudyID) # Here, we only have one aVSA value per speaker and condition

brms::get_prior(
  formula = brms_aVSA_modelFormula,
  data = modelData,
  family = lognormal())
```
Now we can specify weakly informative priors.
```{r}
# specify priors in log space
priors_aVSA <- c(
  prior(normal(0, 10), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(cauchy(0, 10), class = sd),
  prior(cauchy(0, 10), class = sigma)
)
```

### Building the model
```{r}

condition_aVSA <- brms::brm(
  formula = brms_aVSA_modelFormula,
  data = modelData,
  family = lognormal(),
  prior = priors_aVSA,
  chains = 4,
  cores = 4,
  iter = 4000,
  warmup = 1000,
  control = list(adapt_delta = 0.95),
  file = "Models/brms_aVSA.rds",
  file_refit = "on_change"
)
```

### Model Summary
```{r}
summary(condition_aVSA)
tidy(condition_aVSA)
```
### Model Diagnostics
```{r}
plot(condition_aVSA, variables = "^b_")
pp_check(condition_aVSA, ndraws = 1000)

summary(condition_aVSA)$fixed 

plot(conditional_effects(condition_aVSA), ask = FALSE)
```

```{r}
posterior <- as.matrix(condition_aVSA)
fixed_effects <- posterior %>%
  as.data.frame() %>%
  dplyr::select(starts_with("b_")) %>%
  colnames()

mcmc_areas(posterior,
           pars = fixed_effects,
           # arbitrary threshold for shading probability mass
           prob = 0.95) 
```

```{r}
bayestestR::p_direction(condition_aVSA)
```

### Visualizations
```{r}
post_aVSA_data <- modelData %>%
  data_grid(Severity, condition) %>%
  epred_draws(object = condition_aVSA, ndraws = 4000, re_formula = NA)

post_aVSA_plot <- post_aVSA_data %>%
  ggplot() +
  aes(y = .epred,
      x = Severity,
      group = condition,
      fill = condition) +
  
  # Density + CrIs
  stat_halfeye(alpha = .5) +
  
  # Rename axes
  labs(y = "Acoustic VSA (Bark)", x = "Group/Severity")
post_aVSA_plot

aVSA_agg <- modelData %>%
  group_by(StudyID, Severity, condition) %>%
  dplyr::summarise(.epred = mean(aVSA_bark))

final_aVSA_plot <- post_aVSA_plot +
  geom_point(
    data = aVSA_agg,
    aes(
      y = .epred,
      x = Severity,
      group = condition,
      fill = condition
    ),
    pch = 21,
    alpha = .5,
    position = position_nudge(x = -.1)
  )  +
  labs(fill = "Condition")
final_aVSA_plot

ggsave(
  plot = final_aVSA_plot,
  filename = "Plots/brms_aVSA.png",
  height = 3,
  width = 5,
  units = "in",
  scale = 1,
  bg = "white"
)
```

## Kinematic Distance
Kinematic distance was measured from the tongue back sensor (adhered 5 mm from the tongue tip) during the diphthong /ai/ in "buy". The 2D positions of the onset and offset were used to calculate the Euclidean distance. This measure ranges from 0.03 to 28 mm. So, it is bound by 0, and does not have any negative values. For this reason, we use the lognormal family.
### Loading the data
```{r}
aiMeasures <-
  rio::import(file = "../Data/PreppedData/CollatedData/TargetMeasures_aiMeasures.csv") %>%
  dplyr::filter(condition != "lessClear") %>% # We won't use the less clear condition in this study
  
# Here we rename some variables and clean up the factors
  dplyr::mutate(Sex = factor(Sex, levels = c("M", "F")),
                condition = factor(condition, levels = c("conv", "moreClear"))) %>%
  
  # Now we merge the data with the speakerList, which has information about severity level.
  base::merge(., speakerList %>%
                dplyr::select(StudyID, Severity)) %>%
  dplyr::mutate(
    Severity = factor(
      Severity,
      levels = c("HC", "Mild", "Moderate", "Severe", "Profound"),
      labels = c("Control", "Mild", "Moderate", "Severe", "Profound")
    )
  ) %>%
  dplyr::group_by(StudyID, Group, Sex, Severity, condition) %>%
  dplyr::summarise(kinDistance = mean(kinDistance, na.rm = T)) %>%
  dplyr::ungroup()

# Lets visualize the outcome measure, kinDistance
hist(aiMeasures$kinDistance)
hist(log(aiMeasures$kinDistance + 1))

modelData <- aiMeasures %>%
  dplyr::mutate(kinDistance = kinDistance + 1) # applying a constant before the log transformation in the model

performance::check_distribution(modelData$kinDistance)
hist(log(modelData$kinDistance))

# Taking out the trash
rm(aiMeasures)
```

### Priors
First, we need to figure out the model parameters
```{r}

brms_kinDistance_modelFormula <-
  kinDistance ~ Severity * condition +
  (1 | StudyID) # Each Speaker (StudyID) has 2 kinDistance measures for each condition

brms::get_prior(formula = brms_kinDistance_modelFormula,
                data = modelData,
                family = lognormal)
```

Now we can specify weakly informative priors.
```{r}
priors_kinDistance <- c(
  prior(normal(0, 10), class = Intercept),
  prior(normal(0, 10), class = b),
  prior(cauchy(0, 10), class = sigma),
  prior(cauchy(0, 10), class = sd)
)
```

### Building the model
```{r}
condition_kinDistance <- brms::brm(
  formula = brms_kinDistance_modelFormula,
  data = modelData,
  family = lognormal,
  prior = priors_kinDistance,
  chains = 4,
  cores = 4,
  iter = 4000,
  warmup = 1000,
  control = list(adapt_delta = 0.95),
  file = "Models/brms_kinDistance.rds",
  file_refit = "on_change"
)
```

### Model Summary
```{r}
summary(condition_kinDistance)
```

### Model Diagnostics
```{r}
plot(condition_kinDistance, variables = "^b_")
pp_check(condition_kinDistance, ndraws = 1000)

summary(condition_kinDistance)$fixed
plot(conditional_effects(condition_kinDistance), ask = FALSE)
```
```{r}
posterior <- as.matrix(condition_kinDistance)
fixed_effects <- posterior %>%
  as.data.frame() %>%
  dplyr::select(starts_with("b_")) %>%
  colnames()

mcmc_areas(posterior,
           pars = fixed_effects,
           # arbitrary threshold for shading probability mass
           prob = 0.95) 
```

### Visualizations
```{r}
post_kinDistance_data <- modelData %>%
  data_grid(Severity, condition) %>%
  epred_draws(object = condition_kinDistance, ndraws = 4000, re_formula = NA)

post_kinDistance_plot <- post_kinDistance_data %>%
  ggplot() +
  aes(
    y = .epred - 1, # this - 1 removes the constant that was applied before the analysis
    x = Severity,
    group = condition,
    fill = condition
  ) +
  
  # Density + CrIs
  stat_halfeye(alpha = .5) +

  # Rename axes
  labs(
    y = "Kineatic Distance (mm)",
    x = "Severity"
  )
post_kinDistance_plot

kinDistance_agg <- modelData %>%
  group_by(StudyID, Severity, condition) %>%
  dplyr::summarise(.epred = mean(kinDistance - 1))

final_kinDistance_plot <- post_kinDistance_plot +
  geom_point(
    data = kinDistance_agg,
    aes(
      y = .epred,
    x = Severity,
    group = condition,
    #alpha = condition,
    fill = condition),
    pch = 21, alpha = .5,
    position = position_nudge(x = -.1)
  ) +
  coord_cartesian(ylim = c(0, 40)) +
  labs(fill = "Condition")
final_kinDistance_plot

ggsave(
  plot = final_kinDistance_plot,
  filename = "Plots/brms_kinDistance.png",
  height = 3,
  width = 5,
  units = "in",
  scale = 1,
  bg = "white"
)
```

# Model Tables
```{r}
brms_Int <- readRDS("Models/brms_Int.rds") %>%
  summary() %>%
  .$fixed %>%
  as.data.frame()

```

